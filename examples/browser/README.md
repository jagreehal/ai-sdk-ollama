# AI SDK Ollama Browser Example

A modern React-based browser example demonstrating the ai-sdk-ollama provider with AI Elements and shadcn/ui components.

- ‚úÖ **React + AI Elements**: Modern component-based architecture
- ‚úÖ **AI SDK Integration**: Uses `useChat` hook with `toUIMessageStreamResponse`
- ‚úÖ **UI**: shadcn/ui components with Tailwind CSS
- ‚úÖ **Real-time Streaming**: Live text streaming with proper UI message handling
- ‚úÖ **Model Management**: Dynamic model loading and selection
- ‚úÖ **TypeScript**: Full type safety and IntelliSense support

## üìã Prerequisites

1. **Ollama Server**: Make sure Ollama is running locally:

   ```bash
   ollama serve
   ```

2. **Models**: Pull at least one model (llama3.2 recommended):

   ```bash
   ollama pull llama3.2
   ```

3. **No CORS Configuration Needed**: The Vite proxy handles CORS automatically!

## üöÄ Quick Start

### From the monorepo root

```bash
# Install dependencies
pnpm install

# Start the development server
pnpm --filter @examples/browser dev
```

### Or from the browser example directory

```bash
# Navigate to the example
cd examples/browser

# Install dependencies (if not done from root)
pnpm install

# Start the development server
pnpm dev
```

The example will be available at **http://localhost:3000/** (port may vary if 3000 is in use).

## üíª How It Works

### React + AI Elements Architecture

The example uses React with AI Elements for a modern, component-based approach:

```tsx
import { useChat } from '@ai-sdk/react';
import { Conversation, ConversationContent } from './components/ai-elements/conversation';
import { Message, MessageContent } from './components/ai-elements/message';
import { PromptInput, PromptInputTextarea } from './components/ai-elements/prompt-input';

function App() {
  const { messages, sendMessage, status } = useChat();
  
  return (
    <Conversation>
      <ConversationContent>
        {messages.map((message) => (
          <Message key={message.id} from={message.role}>
            <MessageContent>{message.content}</MessageContent>
          </Message>
        ))}
      </ConversationContent>
      <PromptInput onSubmit={handleSubmit}>
        <PromptInputTextarea placeholder="Enter your message..." />
      </PromptInput>
    </Conversation>
  );
}
```

### API Integration with AI Elements

The example uses a custom Vite plugin to handle API routes with proper `toUIMessageStreamResponse`:

```javascript
// vite-api-plugin.ts
export function apiPlugin(): Plugin {
  return {
    name: 'api-plugin',
    configureServer(server) {
      server.middlewares.use('/api/chat', async (req, res, next) => {
        const result = await streamText({
          model: ollama(model),
          messages: convertToModelMessages(messages),
        });
        
        // Use AI Elements' toUIMessageStreamResponse
        const response = result.toUIMessageStreamResponse();
        // Forward response...
      });
    },
  };
}
```

This provides:

- **AI Elements Compatibility**: Proper `toUIMessageStreamResponse` integration
- **Real-time Streaming**: Live UI message streaming
- **No CORS Issues**: Seamless development experience
- **Type Safety**: Full TypeScript support

## üåê Browser vs Node.js

The library automatically selects the correct implementation:

| Environment | Implementation   | Features                                |
| ----------- | ---------------- | --------------------------------------- |
| **Browser** | `ollama/browser` | CORS-aware, fetch-based, no file system |
| **Node.js** | `ollama`         | HTTP client, file system access         |

The API remains **identical** between environments - write once, run anywhere!

## üêõ Troubleshooting

### Connection Issues

- ‚úÖ Ensure Ollama is running: `ollama serve`
- ‚úÖ Check Ollama is accessible at `http://localhost:11434`
- ‚úÖ Verify models are installed: `ollama list`
- ‚úÖ Check browser console for errors

### Model Loading Issues

- ‚úÖ Try refreshing models with the refresh button
- ‚úÖ Check network requests in browser DevTools
- ‚úÖ Verify Vite proxy is working (should see proxy logs)

### Build Issues

- ‚úÖ Build the parent package first: `pnpm build` (from monorepo root)
- ‚úÖ Clear node_modules and reinstall: `rm -rf node_modules && pnpm install`
- ‚úÖ Check for TypeScript errors: `pnpm type-check`
